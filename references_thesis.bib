@Article{educsci13090885,
  AUTHOR = {Akiba, Daisuke and Fraboni, Michelle C.},
  TITLE = {AI-Supported Academic Advising: Exploring ChatGPT’s Current State and Future Potential toward Student Empowerment},
  JOURNAL = {Education Sciences},
  VOLUME = {13},
  YEAR = {2023},
  NUMBER = {9},
  ARTICLE-NUMBER = {885},
  URL = {https://www.mdpi.com/2227-7102/13/9/885},
  ISSN = {2227-7102},
  DOI = {10.3390/educsci13090885}
}

@article{chatgpt_higher_education_ssrn,
  title={ChatGPT for Higher Education and Professional Development: A Guide},
  author={P. S. Aithal and Shubhrajyosna Aithal},
  journal={SSRN},
  year={2024},
  pages={1-17, 41-48, 79-88}
}

@techreport{banerjee_2024,
  title={Customizing Large Language Models for Automated Academic Advising at Universities},
  author={Banerjee, Ronit and Butziger, Kathryn and Filizzola Ortiz, Jose Fabrizio and Kiszla, Matthew},
  institution={Worcester Polytechnic Institute},
  year={2024}
}

@Article{chukhlomin2024,
  AUTHOR = {Chukhlomin, Valeri},
  TITLE = {Exploring the Use of Custom GPTs in Higher Education Strategic Planning: A Preliminary Field Report},
  JOURNAL = {SSRN},
  YEAR = {2024},
  URL = {https://ssrn.com/abstract=4793697},
  DOI = {10.2139/ssrn.4793697}
}

@Article{dai2023,
  AUTHOR = {Dai, Yun and Liu, Ang and Lim, Cher Ping},
  TITLE = {Reconceptualizing ChatGPT and generative AI as a student-driven innovation in higher education},
  JOURNAL = {Procedia CIRP},
  VOLUME = {119},
  YEAR = {2023},
  PAGES = {84-90},
  ISSN = {2212-8271},
  DOI = {10.1016/j.procir.2023.05.002},
  URL = {https://www.sciencedirect.com/science/article/pii/S2212827123004407}
}

@Article{arxiv230518086,
  AUTHOR = {Gabashvili, Irene S.},
  TITLE = {The impact and applications of ChatGPT: a systematic review of literature reviews},
  JOURNAL = {arXiv},
  VOLUME = {2305.18086},
  YEAR = {2023},
  URL = {https://doi.org/10.48550/arXiv.2305.18086}
}

@Article{bdcc7020062,
  AUTHOR = {Hassani, Hossein and Silva, Emmanuel Sirmal},
  TITLE = {The Role of ChatGPT in Data Science: How AI-Assisted Conversational Interfaces Are Revolutionizing the Field},
  JOURNAL = {Big Data and Cognitive Computing},
  VOLUME = {7},
  YEAR = {2023},
  NUMBER = {2},
  ARTICLE-NUMBER = {62},
  URL = {https://www.mdpi.com/2504-2289/7/2/62},
  ISSN = {2504-2289},
  DOI = {10.3390/bdcc7020062}
}

@Article{su15075614,
  AUTHOR = {Kooli, Chokri},
  TITLE = {Chatbots in Education and Research: A Critical Examination of Ethical Implications and Solutions},
  JOURNAL = {Sustainability},
  VOLUME = {15},
  YEAR = {2023},
  NUMBER = {7},
  ARTICLE-NUMBER = {5614},
  URL = {https://www.mdpi.com/2071-1050/15/7/5614},
  ISSN = {2071-1050},
  DOI = {10.3390/su15075614}
}

@Article{labadze2023,
  AUTHOR = {Labadze, L. and Grigolia, M. and Machaidze, L.},
  TITLE = {Role of AI chatbots in education: systematic literature review},
  JOURNAL = {Int J Educ Technol High Educ},
  VOLUME = {20},
  YEAR = {2023},
  PAGES = {56},
  DOI = {10.1186/s41239-023-00426-1},
  URL = {https://doi.org/10.1186/s41239-023-00426-1}
    
}

@inproceedings{lekan2024aiaugmented,
  title={{AI}-Augmented Advising: {AI}-Augmented Advising: A Comparative Study of Chat{GPT}-4 and Advisor-based Major Recommendations},
  author={Kasra Lekan and Zachary Pardos},
  booktitle={AI for Education: Bridging Innovation and Responsibility at the 38th AAAI Annual Conference on AI},
  year={2024},
  url={https://openreview.net/forum?id=ZHidmlDWJs}
}var

@Misc{openai2023,
  AUTHOR = {OpenAI},
  TITLE = {OpenAI Documentation},
  HOWPUBLISHED = {\url{https://platform.openai.com/docs/overview}},
  YEAR = {2023} 
}

@Article{STEELE2023100160,
  TITLE = {To GPT or not GPT? Empowering our students to learn with AI},
  JOURNAL = {Computers and Education: Artificial Intelligence},
  VOLUME = {5},
  PAGES = {100160},
  YEAR = {2023},
  ISSN = {2666-920X},
  DOI = {https://doi.org/10.1016/j.caeai.2023.100160},
  URL = {https://www.sciencedirect.com/science/article/pii/S2666920X23000395},
  AUTHOR = {Jennifer L. Steele},
  KEYWORDS = {ChatGPT, Artificial intelligence, Pedagogy, Educational technology, Academic integrity},
  ABSTRACT = {I argue that ChatGPT and other generative artificial intelligence tools pose three main threats to our current education systems, creating problems of measurement, information accuracy, and skill devaluation. But when we place these threats into historical context, we see that AI tools can also empower students and level the educational playing field. In classrooms from primary to tertiary and spanning all content areas, we can help our students become critical thinkers by using ChatGPT to comprehend texts, aggregate knowledge, and understand genre conventions in prose as well as programming. The aim is to help students leverage AI as a tool that they question and critique, advancing their own comprehension, research, and composition skills in the process.} 
}

@Article{maniar2023,
  AUTHOR = {Maniar, Himanshu},
  TITLE = {An Impact of Chat GPT in Higher Education},
  JOURNAL = {International Journal of Innovative Science, Engineering Technology},
  VOLUME = {10},
  NUMBER = {08},
  YEAR = {2023},
  ISSN = {2348-7968},
  URL = {https://www.ijiset.com/vol10issue8/}
}

@article{thottoli2024robo,
  title={Robo academic advisor: Can chatbots and artificial intelligence replace human interaction?},
  author={Thottoli, Mohammed Muneerali and Alruqaishi, Badria Hamed and Soosaimanickam, Arockiasamy},
  journal={Contemporary Educational Technology},
  volume={16},
  number={1},
  pages={ep485},
  year={2024},
  doi={10.30935/cedtech/13498},
  publisher={University of Nizwa},
  note={Corresponding author: muneerali@unizwa.edu.om}
}



@inproceedings{Merikko2024AnAA,
  title={An AI Agent Facilitating Student Help-Seeking: Producing Data on Student Support Needs},
  author={Merikko, Joonas and Silvola, Anni},
  booktitle={Joint Proceedings of LAK 2024 Workshops, co-located with 14th International Conference on Learning Analytics and Knowledge (LAK 2024)},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:269292605}
}`

@misc{inaba2024largelanguagemodelsused,
  title={Can Large Language Models be Used to Provide Psychological Counselling? An Analysis of GPT-4-Generated Responses Using Role-play Dialogues},
  author={Michimasa Inaba and Mariko Ukiyo and Keiko Takamizo},
  year={2024},
  eprint={2402.12738},
  archivePrefix={arXiv},
  primaryClass={cs.CL},
  url={https://arxiv.org/abs/2402.12738}
}



@INPROCEEDINGS{9099191,
  author={Alkhoori, Abdulrahman and Kuhail, Mohammad Amin and Alkhoori, Abdulla},
  booktitle={2020 12th Annual Undergraduate Research Conference on Applied Computing (URC)}, 
  title={UniBud: A Virtual Academic Adviser}, 
  year={2020},
  pages={1-4},
  keywords={Usability;Task analysis;Expert systems;Databases;Planning;Natural language processing;voice-based interaction;natural language processing;user experience},
  doi={10.1109/URC49805.2020.9099191}
} 

@misc{han2023understandingincontextlearningsupportive,
      title={Understanding In-Context Learning via Supportive Pretraining Data}, 
      author={Xiaochuang Han and Daniel Simig and Todor Mihaylov and Yulia Tsvetkov and Asli Celikyilmaz and Tianlu Wang},
      year={2023},
      eprint={2306.15091},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2306.15091}, 
}

@article{Islam_2024,
title={GPT-4o: The Cutting-Edge Advancement in Multimodal LLM},
url={http://dx.doi.org/10.36227/techrxiv.171986596.65533294/v1},
DOI={10.36227/techrxiv.171986596.65533294/v1},
publisher={Institute of Electrical and Electronics Engineers (IEEE)},
author={Islam, Raisa and Moushi, Owana Marzia},
year={2024},
month=jul }

@Article{D4SC00924J,
author ="Zhang, Wei and Wang, Qinggong and Kong, Xiangtai and Xiong, Jiacheng and Ni, Shengkun and Cao, Duanhua and Niu, Buying and Chen, Mingan and Li, Yameng and Zhang, Runze and Wang, Yitian and Zhang, Lehan and Li, Xutong and Xiong, Zhaoping and Shi, Qian and Huang, Ziming and Fu, Zunyun and Zheng, Mingyue",
title  ="Fine-tuning large language models for chemical text mining",
journal  ="Chem. Sci.",
year  ="2024",
volume  ="15",
issue  ="27",
pages  ="10600-10611",
publisher  ="The Royal Society of Chemistry",
doi  ="10.1039/D4SC00924J",
url  ="http://dx.doi.org/10.1039/D4SC00924J",
abstract  ="Extracting knowledge from complex and diverse chemical texts is a pivotal task for both experimental and computational chemists. The task is still considered to be extremely challenging due to the complexity of the chemical language and scientific literature. This study explored the power of fine-tuned large language models (LLMs) on five intricate chemical text mining tasks: compound entity recognition{,} reaction role labelling{,} metal–organic framework (MOF) synthesis information extraction{,} nuclear magnetic resonance spectroscopy (NMR) data extraction{,} and the conversion of reaction paragraphs to action sequences. The fine-tuned LLMs demonstrated impressive performance{,} significantly reducing the need for repetitive and extensive prompt engineering experiments. For comparison{,} we guided ChatGPT (GPT-3.5-turbo) and GPT-4 with prompt engineering and fine-tuned GPT-3.5-turbo as well as other open-source LLMs such as Mistral{,} Llama3{,} Llama2{,} T5{,} and BART. The results showed that the fine-tuned ChatGPT models excelled in all tasks. They achieved exact accuracy levels ranging from 69% to 95% on these tasks with minimal annotated data. They even outperformed those task-adaptive pre-training and fine-tuning models that were based on a significantly larger amount of in-domain data. Notably{,} fine-tuned Mistral and Llama3 show competitive abilities. Given their versatility{,} robustness{,} and low-code capability{,} leveraging fine-tuned LLMs as flexible and effective toolkits for automated data acquisition could revolutionize chemical knowledge extraction."}

@misc{taherkhani2024epiccosteffectivesearchbasedprompt,
      title={EPiC: Cost-effective Search-based Prompt Engineering of LLMs for Code Generation}, 
      author={Hamed Taherkhani and Melika Sepindband and Hung Viet Pham and Song Wang and Hadi Hemmati},
      year={2024},
      eprint={2408.11198},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2408.11198}, 
}

@article{Liu_2023,
   title={Summary of ChatGPT-Related research and perspective towards the future of large language models},
   volume={1},
   ISSN={2950-1628},
   url={http://dx.doi.org/10.1016/j.metrad.2023.100017},
   DOI={10.1016/j.metrad.2023.100017},
   number={2},
   journal={Meta-Radiology},
   publisher={Elsevier BV},
   author={Liu, Yiheng and Han, Tianle and Ma, Siyuan and Zhang, Jiayue and Yang, Yuanyuan and Tian, Jiaming and He, Hao and Li, Antong and He, Mengshen and Liu, Zhengliang and Wu, Zihao and Zhao, Lin and Zhu, Dajiang and Li, Xiang and Qiang, Ning and Shen, Dingang and Liu, Tianming and Ge, Bao},
   year={2023},
   month=sep, pages={100017} }


@article{Zhang2024Comparison,
  title     = {Comparison of Prompt Engineering and Fine-Tuning Strategies in Large Language Models in the Classification of Clinical Notes},
  author    = {Zhang, Xiaodan and Talukdar, Nabasmita and Vemulapalli, Sandeep and Ahn, Sumyeong and Wang, Jiankun and Meng, Han and Murtaza, Sardar Mehtab Bin and Leshchiner, Dmitry and Dave, Aakash Ajay and Joseph, Dimitri F and Witteveen-Lane, Martin and Chesla, Dave and Zhou, Jiayu and Chen, Bin},
  journal   = {medRxiv},
  year      = {2024},
  month     = {Feb},
  day       = {8},
  doi       = {10.1101/2024.02.07.24302444},
  url       = {https://doi.org/10.1101/2024.02.07.24302444},
}

@misc{openai_finetuning_guide,
  title = {Fine-Tuning Guide},
  author = {{OpenAI}},
  year = {2024},
  url = {https://platform.openai.com/docs/guides/fine-tuning},
  note = {Accessed: 2024-10-16}
}

@misc{PyPDF2,
  title        = {PyPDF2},
  howpublished = {\url{https://pypi.org/project/PyPDF2/}},
  note         = {Retrieved March 1, 2025}
  
}


}
@misc{huggingface_models_libraries,
  author = {Hugging Face},
  title = {Hugging Face},
  howpublished = {\url{https://huggingface.com/}},
  note = {Visited on 11/17/2023}
}



@misc{nltkSentiment,
  author = {Natural Language Toolkit},
  title = {How-to: Sentiment},
  howpublished = {\url{https://www.nltk.org/howto/sentiment.html}},
  note = {Retrieved February 6, 2025}
}


@misc{pythonUUID,
  title        = {uuid -- UUID objects},
  author       = {{Python Software Foundation}},
  howpublished = {\url{https://docs.python.org/3/library/uuid.html}},
  note         = { Retrieved March 1, 2025} 
  
}


@misc{tiktoken033,
  title = {tiktoken Package},
  note = {tiktoken 0.3.3},
  howpublished = {\url{https://pypi.org/project/tiktoken/0.3.3}}
}

@article{bergemann2025economics,
  title={The Economics of Large Language Models: Token Allocation, Fine-Tuning, and Optimal Pricing},
  author={Bergemann, Dirk and Bonatti, Alessandro and Smolin, Alex},
  journal={arXiv preprint arXiv:2502.07736},
  year={2025}
}

@article{jeong2024fine,
  title={Fine-tuning and utilization methods of domain-specific llms},
  author={Jeong, Cheonsu},
  journal={arXiv preprint arXiv:2401.02981},
  year={2024}
}

@inproceedings{lin2024data,
  title={Data-efficient Fine-tuning for LLM-based Recommendation},
  author={Lin, Xinyu and Wang, Wenjie and Li, Yongqi and Yang, Shuo and Feng, Fuli and Wei, Yinwei and Chua, Tat-Seng},
  booktitle={Proceedings of the 47th international ACM SIGIR conference on research and development in information retrieval},
  pages={365--374},
  year={2024}
}

@article{jo2025efficiency,
  title={Efficiency and Performance Optimization in Large Language Models through IB Fine-Tuning},
  author={Jo, Ashly Ann and Raj, Ebin Deni and Sahoo, Jayakrushna},
  journal={ACM Transactions on Intelligent Systems and Technology},
  year={2025},
  publisher={ACM New York, NY}
}

@article{anisuzzaman2024fine,
  title={Fine-tuning llms for specialized use cases},
  author={Anisuzzaman, DM and Malins, Jeffrey G and Friedman, Paul A and Attia, Zachi I},
  journal={Mayo Clinic Proceedings: Digital Health},
  year={2024},
  publisher={Elsevier}
}

@misc{pareja2024unveilingsecretrecipeguide,
      title={Unveiling the Secret Recipe: A Guide For Supervised Fine-Tuning Small LLMs}, 
      author={Aldo Pareja and Nikhil Shivakumar Nayak and Hao Wang and Krishnateja Killamsetty and Shivchander Sudalairaj and Wenlong Zhao and Seungwook Han and Abhishek Bhandwaldar and Guangxuan Xu and Kai Xu and Ligong Han and Luke Inglis and Akash Srivastava},
      year={2024},
      eprint={2412.13337},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2412.13337}, 
}

@misc{touvron2023llama2openfoundation,
      title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, 
      author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
      year={2023},
      eprint={2307.09288},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.09288}, 
}

@article{pokhrel2024building,
  title={Building Customized Chatbots for Document Summarization and Question Answering using Large Language Models using a Framework with OpenAI, Lang chain, and Streamlit},
  author={Pokhrel, Sangita and Ganesan, Swathi and Akther, Tasnim and Karunarathne, Lakmali and others},
  journal={Journal of Information Technology and Digital World},
  volume={6},
  number={1},
  pages={70--86},
  year={2024},
  publisher={IRO Journals}
}

@phdthesis{gunnam2024performance,
  title={The Performance and AI Optimization Issues for Task-Oriented Chatbots},
  author={Gunnam, Ganesh Reddy},
  year={2024},
  school={The University of Texas at San Antonio}
}

@misc{openai2024,
  author = {OpenAI},
  title = {GPT‑4o system card},
  year = {2024},
  month = {August},
  day = {8},
  url = {https://openai.com/index/gpt-4o-system-card/}
}

@misc{openaiTextGeneration,
  author = {OpenAI},
  title = {Text Generation},
  howpublished = {\url{https://platform.openai.com/docs/guides/text-generation}},
  note = {n.d.}
}

@misc{openaiAPIPricing,
  author = {OpenAI},
  title = {API pricing},
  howpublished = {\url{https://openai.com/api/pricing/}},
  note = {n.d.}
}

@article{tan2024fine,
  title={Fine-tuning large language model (llm) artificial intelligence chatbots in ophthalmology and llm-based evaluation using GPT-4},
  author={Tan, Ting Fang and Elangovan, Kabilan and Jin, Liyuan and Jie, Yao and Yong, Li and Lim, Joshua and Poh, Stanley and Ng, Wei Yan and Lim, Daniel and Ke, Yuhe and others},
  journal={arXiv preprint arXiv:2402.10083},
  year={2024}
}

@inproceedings{ha2023meta,
  title={Meta-Learning of Prompt Generation for Lightweight Prompt Engineering on Language-Model-as-a-Service},
  author={Ha, Hyeonmin and Lee, Jihye and Han, Wookje and Chun, Byung-Gon},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={2433--2445},
  year={2023}
}

@article{han2024token,
  title={Token-budget-aware llm reasoning},
  author={Han, Tingxu and Fang, Chunrong and Zhao, Shiyu and Ma, Shiqing and Chen, Zhenyu and Wang, Zhenting},
  journal={arXiv preprint arXiv:2412.18547},
  year={2024}
}

@article{shekhar2024towards,
  title={Towards optimizing the costs of llm usage},
  author={Shekhar, Shivanshu and Dubey, Tanishq and Mukherjee, Koyel and Saxena, Apoorv and Tyagi, Atharv and Kotla, Nishanth},
  journal={arXiv preprint arXiv:2402.01742},
  year={2024}
}


@Article{robotics13050068,
AUTHOR = {Villa, Laura and Carneros-Prado, David and Dobrescu, Cosmin C. and Sánchez-Miguel, Adrián and Cubero, Guillermo and Hervás, Ramón},
TITLE = {Comparative Analysis of Generic and Fine-Tuned Large Language Models for Conversational Agent Systems},
JOURNAL = {Robotics},
VOLUME = {13},
YEAR = {2024},
NUMBER = {5},
ARTICLE-NUMBER = {68},
URL = {https://www.mdpi.com/2218-6581/13/5/68},
ISSN = {2218-6581},
ABSTRACT = {In the rapidly evolving domain of conversational agents, the integration of Large Language Models (LLMs) into Chatbot Development Platforms (CDPs) is a significant innovation. This study compares the efficacy of employing generic and fine-tuned GPT-3.5-turbo models for designing dialog flows, focusing on the intent and entity recognition crucial for dynamic conversational interactions. Two distinct approaches are introduced: a generic GPT-based system (G-GPT) leveraging the pre-trained model with complex prompts for intent and entity detection, and a fine-tuned GPT-based system (FT-GPT) employing customized models for enhanced specificity and efficiency. The evaluation encompassed the systems’ ability to accurately classify intents and recognize named entities, contrasting their adaptability, operational efficiency, and customization capabilities. The results revealed that, while the G-GPT system offers ease of deployment and versatility across various contexts, the FT-GPT system demonstrates superior precision, efficiency, and customization, although it requires initial training and dataset preparation. This research highlights the versatility of LLMs in enriching conversational features for talking assistants, from social robots to interactive chatbots. By tailoring these advanced models, the fluidity and responsiveness of conversational agents can be enhanced, making them more adaptable and effective in a variety of settings, from customer service to interactive learning environments.},
DOI = {10.3390/robotics13050068}
}

@misc{mysql_connector_python_example_connecting,
  author = {{MySQL}},
  title = {Example: Connecting},
  year = {n.d.},
  howpublished = {\url{https://dev.mysql.com/doc/connector-python/en/connector-python-example-connecting.html}},
  note = {Accessed December 2, 2024}
}

@article{Page2017Pounce,
  author       = {Lindsay C. Page and Hunter Gehlbach},
  title        = {How an Artificially Intelligent Virtual Assistant Helps Students Navigate the Road to College},
  journal      = {AERA Open},
  year         = {2017},
  volume       = {3},
  number       = {4},
  pages        = {1--12},
  doi          = {10.1177/2332858417749220},
  url          = {https://doi.org/10.1177/2332858417749220},
  publisher    = {SAGE Publications},
  note         = {Creative Commons Attribution-NonCommercial 4.0 License}
}
