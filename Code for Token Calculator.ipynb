{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdb9d848-13bd-48f2-a29b-284f1a8d158d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 25958\n",
      "Equivalent price: $0.7787\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "\n",
    "def main():\n",
    "    # Step 1: Read and parse the JSON file.\n",
    "    with open(\"2024_2025_spring_courses.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Step 2: Convert the JSON data to a string.\n",
    "    json_text = json.dumps(data)\n",
    "    \n",
    "    # Step 3: Explicitly get the encoding.\n",
    "    # Since \"GPT-4o\" is not automatically mapped, we'll use \"cl100k_base\" as the encoding.\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    # Step 4: Encode the text and count the tokens.\n",
    "    tokens = encoding.encode(json_text)\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    # Step 5: Calculate the equivalent price.\n",
    "    # Example rate: $0.03 per 1,000 tokens\n",
    "    price_per_1000_tokens = 0.03  \n",
    "    equivalent_price = (token_count / 1000) * price_per_1000_tokens\n",
    "    \n",
    "    print(f\"Token count: {token_count}\")\n",
    "    print(f\"Equivalent price: ${equivalent_price:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "594a187a-0fcf-4c28-a67b-162bb01fc46e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token and Character Statistics:\n",
      "---------------------------------\n",
      "Total characters: 104335\n",
      "Total words: 12160\n",
      "Total tokens (using 'cl100k_base' encoding): 25958\n",
      "Average characters per token: 4.02\n",
      "\n",
      "Cost Calculation:\n",
      "-----------------\n",
      "Price per 1000 tokens: $0.03\n",
      "Equivalent price: $0.7787\n",
      "\n",
      "Calculation Details:\n",
      "---------------------\n",
      "1. The JSON file is read and serialized using json.dumps(), and its length is used for the character count.\n",
      "2. Word count is determined by splitting the text on whitespace (using str.split()).\n",
      "3. Tokens are calculated by encoding the text with tiktoken's 'cl100k_base' encoding.\n",
      "4. Average characters per token is: total characters / total tokens.\n",
      "5. The cost is computed by scaling the token count to a price per 1,000 tokens.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "\n",
    "def main():\n",
    "    # Step 1: Read and parse the JSON file.\n",
    "    with open(\"2024_2025_spring_courses.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Step 2: Convert the JSON data to a string.\n",
    "    # This serialized string is what we will tokenize.\n",
    "    json_text = json.dumps(data)\n",
    "    \n",
    "    # Step 3: Calculate the total number of characters.\n",
    "    char_count = len(json_text)\n",
    "    \n",
    "    # Step 4: Calculate the total number of words.\n",
    "    # Here, we split the text by whitespace.\n",
    "    word_count = len(json_text.split())\n",
    "    \n",
    "    # Step 5: Get the tokenizer encoding.\n",
    "    # Since GPT-4o isn't directly available, we use \"cl100k_base\" which is common for GPT-4.\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    # Step 6: Encode the text and count tokens.\n",
    "    tokens = encoding.encode(json_text)\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    # Calculate the average number of characters per token.\n",
    "    avg_chars_per_token = char_count / token_count if token_count > 0 else 0\n",
    "    \n",
    "    # Step 7: Calculate the equivalent price.\n",
    "    # Example rate: $0.03 per 1,000 tokens.\n",
    "    price_per_1000_tokens = 0.03\n",
    "    equivalent_price = (token_count / 1000) * price_per_1000_tokens\n",
    "    \n",
    "    # Step 8: Print out all details.\n",
    "    print(\"Token and Character Statistics:\")\n",
    "    print(\"---------------------------------\")\n",
    "    print(f\"Total characters: {char_count}\")\n",
    "    print(f\"Total words: {word_count}\")\n",
    "    print(f\"Total tokens (using 'cl100k_base' encoding): {token_count}\")\n",
    "    print(f\"Average characters per token: {avg_chars_per_token:.2f}\")\n",
    "    print()\n",
    "    print(\"Cost Calculation:\")\n",
    "    print(\"-----------------\")\n",
    "    print(f\"Price per 1000 tokens: ${price_per_1000_tokens}\")\n",
    "    print(f\"Equivalent price: ${equivalent_price:.4f}\")\n",
    "    print()\n",
    "    print(\"Calculation Details:\")\n",
    "    print(\"---------------------\")\n",
    "    print(\"1. The JSON file is read and serialized using json.dumps(), and its length is used for the character count.\")\n",
    "    print(\"2. Word count is determined by splitting the text on whitespace (using str.split()).\")\n",
    "    print(\"3. Tokens are calculated by encoding the text with tiktoken's 'cl100k_base' encoding.\")\n",
    "    print(\"4. Average characters per token is: total characters / total tokens.\")\n",
    "    print(\"5. The cost is computed by scaling the token count to a price per 1,000 tokens.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2edcffe-4629-4992-8fde-d4dddf7b40b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token and Character Statistics:\n",
      "---------------------------------\n",
      "Total characters: 104335\n",
      "Total words: 12160\n",
      "Total tokens (using 'cl100k_base' encoding): 25958\n",
      "Average characters per token: 4.02\n",
      "\n",
      "Cost Calculation:\n",
      "-----------------\n",
      "Blended price per 1,000 tokens: $0.0040\n",
      "Equivalent price for 25958 tokens: $0.1038\n",
      "\n",
      "Calculation Details:\n",
      "---------------------\n",
      "1. The JSON file is read and serialized using json.dumps(), and its length is used for the character count.\n",
      "2. Word count is determined by splitting the text on whitespace (using str.split()).\n",
      "3. Tokens are calculated by encoding the text with tiktoken's 'cl100k_base' encoding.\n",
      "4. Average characters per token is computed as: total characters / total tokens.\n",
      "5. The cost is computed using a blended rate of $4.00 per 1,000,000 tokens (or $0.004 per 1,000 tokens),\n",
      "   which assumes 80% input and 20% output tokens for the GPT-4o model.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "\n",
    "def main():\n",
    "    # Step 1: Read and parse the JSON file.\n",
    "    with open(\"2024_2025_spring_courses.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Step 2: Convert the JSON data to a string.\n",
    "    # This serialized string is what we will tokenize.\n",
    "    json_text = json.dumps(data)\n",
    "    \n",
    "    # Step 3: Calculate the total number of characters.\n",
    "    char_count = len(json_text)\n",
    "    \n",
    "    # Step 4: Calculate the total number of words.\n",
    "    # Here, we split the text by whitespace.\n",
    "    word_count = len(json_text.split())\n",
    "    \n",
    "    # Step 5: Get the tokenizer encoding.\n",
    "    # Since GPT-4o isn't directly available, we use \"cl100k_base\",\n",
    "    # which is common for GPT-4 based models.\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    # Step 6: Encode the text and count tokens.\n",
    "    tokens = encoding.encode(json_text)\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    # Calculate the average number of characters per token.\n",
    "    avg_chars_per_token = char_count / token_count if token_count > 0 else 0\n",
    "    \n",
    "    # Step 7: Calculate the equivalent price.\n",
    "    # Using the blended pricing for GPT-4o:\n",
    "    # - Input tokens: $2.50 per 1M tokens\n",
    "    # - Output tokens: $10.00 per 1M tokens\n",
    "    # Assuming 80% of tokens are input and 20% output:\n",
    "    #   Blended cost = (0.8 * 2.50) + (0.2 * 10.00) = $4.00 per 1M tokens,\n",
    "    # which is equivalent to $0.004 per 1,000 tokens.\n",
    "    price_per_1000_tokens = 0.004  # $0.004 per 1,000 tokens\n",
    "    equivalent_price = (token_count / 1000) * price_per_1000_tokens\n",
    "    \n",
    "    # Step 8: Print out all details.\n",
    "    print(\"Token and Character Statistics:\")\n",
    "    print(\"---------------------------------\")\n",
    "    print(f\"Total characters: {char_count}\")\n",
    "    print(f\"Total words: {word_count}\")\n",
    "    print(f\"Total tokens (using 'cl100k_base' encoding): {token_count}\")\n",
    "    print(f\"Average characters per token: {avg_chars_per_token:.2f}\")\n",
    "    print()\n",
    "    print(\"Cost Calculation:\")\n",
    "    print(\"-----------------\")\n",
    "    print(f\"Blended price per 1,000 tokens: ${price_per_1000_tokens:.4f}\")\n",
    "    print(f\"Equivalent price for {token_count} tokens: ${equivalent_price:.4f}\")\n",
    "    print()\n",
    "    print(\"Calculation Details:\")\n",
    "    print(\"---------------------\")\n",
    "    print(\"1. The JSON file is read and serialized using json.dumps(), and its length is used for the character count.\")\n",
    "    print(\"2. Word count is determined by splitting the text on whitespace (using str.split()).\")\n",
    "    print(\"3. Tokens are calculated by encoding the text with tiktoken's 'cl100k_base' encoding.\")\n",
    "    print(\"4. Average characters per token is computed as: total characters / total tokens.\")\n",
    "    print(\"5. The cost is computed using a blended rate of $4.00 per 1,000,000 tokens (or $0.004 per 1,000 tokens),\")\n",
    "    print(\"   which assumes 80% input and 20% output tokens for the GPT-4o model.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "694bde1e-54a7-4797-b7a3-147c7cdb5d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token and Character Statistics:\n",
      "---------------------------------\n",
      "Total characters: 104335\n",
      "Total words: 12160\n",
      "Total input tokens (using 'cl100k_base' encoding): 25958\n",
      "Average characters per token: 4.02\n",
      "\n",
      "Input Cost Calculation:\n",
      "-------------------------\n",
      "Price per 1,000,000 input tokens: $2.50\n",
      "Equivalent cost for 25958 input tokens: $0.0649\n",
      "\n",
      "Calculation Details:\n",
      "---------------------\n",
      "1. The JSON file is read and serialized using json.dumps(), and its length is used for the character count.\n",
      "2. Word count is determined by splitting the text on whitespace (using str.split()).\n",
      "3. Input tokens are calculated by encoding the text with tiktoken's 'cl100k_base' encoding.\n",
      "4. The cost is computed using the input pricing of $2.50 per 1,000,000 tokens.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "\n",
    "def main():\n",
    "    # Step 1: Read and parse the JSON file.\n",
    "    with open(\"2024_2025_spring_courses.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Step 2: Convert the JSON data to a string.\n",
    "    # This serialized string is what we will tokenize.\n",
    "    json_text = json.dumps(data)\n",
    "    \n",
    "    # Step 3: Calculate the total number of characters.\n",
    "    char_count = len(json_text)\n",
    "    \n",
    "    # Step 4: Calculate the total number of words.\n",
    "    # Splitting the text by whitespace gives an approximate word count.\n",
    "    word_count = len(json_text.split())\n",
    "    \n",
    "    # Step 5: Get the tokenizer encoding.\n",
    "    # We use \"cl100k_base\" encoding which is common for GPT-4 and its variants.\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    \n",
    "    # Step 6: Encode the text and count the tokens (these represent input tokens).\n",
    "    tokens = encoding.encode(json_text)\n",
    "    token_count = len(tokens)\n",
    "    \n",
    "    # Calculate the average number of characters per token.\n",
    "    avg_chars_per_token = char_count / token_count if token_count > 0 else 0\n",
    "    \n",
    "    # Step 7: Calculate the equivalent cost for input tokens only.\n",
    "    # Given pricing: $2.50 per 1,000,000 tokens\n",
    "    # Therefore, cost per token = 2.50 / 1,000,000 dollars.\n",
    "    input_cost_per_token = 2.50 / 1_000_000  # dollars per token\n",
    "    equivalent_price_input = token_count * input_cost_per_token\n",
    "    \n",
    "    # Step 8: Print out all details.\n",
    "    print(\"Token and Character Statistics:\")\n",
    "    print(\"---------------------------------\")\n",
    "    print(f\"Total characters: {char_count}\")\n",
    "    print(f\"Total words: {word_count}\")\n",
    "    print(f\"Total input tokens (using 'cl100k_base' encoding): {token_count}\")\n",
    "    print(f\"Average characters per token: {avg_chars_per_token:.2f}\")\n",
    "    print()\n",
    "    print(\"Input Cost Calculation:\")\n",
    "    print(\"-------------------------\")\n",
    "    print(f\"Price per 1,000,000 input tokens: $2.50\")\n",
    "    print(f\"Equivalent cost for {token_count} input tokens: ${equivalent_price_input:.4f}\")\n",
    "    print()\n",
    "    print(\"Calculation Details:\")\n",
    "    print(\"---------------------\")\n",
    "    print(\"1. The JSON file is read and serialized using json.dumps(), and its length is used for the character count.\")\n",
    "    print(\"2. Word count is determined by splitting the text on whitespace (using str.split()).\")\n",
    "    print(\"3. Input tokens are calculated by encoding the text with tiktoken's 'cl100k_base' encoding.\")\n",
    "    print(\"4. The cost is computed using the input pricing of $2.50 per 1,000,000 tokens.\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afeeef06-a9d7-45c4-aafe-ed5c3481e496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: streamlit in /opt/anaconda3/lib/python3.11/site-packages (1.30.0)\n",
      "Requirement already satisfied: tiktoken in /opt/anaconda3/lib/python3.11/site-packages (0.7.0)\n",
      "Requirement already satisfied: altair<6,>=4.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (5.0.1)\n",
      "Requirement already satisfied: blinker<2,>=1.0.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (1.6.2)\n",
      "Requirement already satisfied: cachetools<6,>=4.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (5.3.3)\n",
      "Requirement already satisfied: click<9,>=7.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (8.1.7)\n",
      "Requirement already satisfied: importlib-metadata<8,>=1.4 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (7.0.0)\n",
      "Requirement already satisfied: numpy<2,>=1.19.3 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (1.26.4)\n",
      "Collecting packaging<24,>=16.8 (from streamlit)\n",
      "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: pandas<3,>=1.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (2.0.3)\n",
      "Requirement already satisfied: pillow<11,>=7.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (10.2.0)\n",
      "Requirement already satisfied: protobuf<5,>=3.20 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (3.20.3)\n",
      "Requirement already satisfied: pyarrow>=6.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (14.0.2)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.3 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (2.8.2)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (2.31.0)\n",
      "Requirement already satisfied: rich<14,>=10.14.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (13.3.5)\n",
      "Requirement already satisfied: tenacity<9,>=8.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (8.2.3)\n",
      "Requirement already satisfied: toml<2,>=0.10.1 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (0.10.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (4.12.2)\n",
      "Requirement already satisfied: tzlocal<6,>=1.1 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (2.1)\n",
      "Requirement already satisfied: validators<1,>=0.2 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (0.18.2)\n",
      "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (3.1.37)\n",
      "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (0.8.0)\n",
      "Requirement already satisfied: tornado<7,>=6.0.3 in /opt/anaconda3/lib/python3.11/site-packages (from streamlit) (6.3.3)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.11/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (3.1.3)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /opt/anaconda3/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
      "Requirement already satisfied: toolz in /opt/anaconda3/lib/python3.11/site-packages (from altair<6,>=4.0->streamlit) (0.12.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/anaconda3/lib/python3.11/site-packages (from importlib-metadata<8,>=1.4->streamlit) (3.17.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas<3,>=1.3.0->streamlit) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/anaconda3/lib/python3.11/site-packages (from pandas<3,>=1.3.0->streamlit) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.11/site-packages (from python-dateutil<3,>=2.7.3->streamlit) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.11/site-packages (from requests<3,>=2.27->streamlit) (2024.7.4)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.11/site-packages (from rich<14,>=10.14.0->streamlit) (2.15.1)\n",
      "Requirement already satisfied: decorator>=3.4.0 in /opt/anaconda3/lib/python3.11/site-packages (from validators<1,>=0.2->streamlit) (5.1.1)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in /opt/anaconda3/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.11/site-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.10.6)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.0)\n",
      "Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "\u001b[33mWARNING: Error parsing dependencies of mermaid: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    torch (>=1.7torchvision)\n",
      "          ~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: packaging\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 24.1\n",
      "    Uninstalling packaging-24.1:\n",
      "      Successfully uninstalled packaging-24.1\n",
      "Successfully installed packaging-23.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit tiktoken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6e89125-7504-4cc6-8cac-092b6722f081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 1700\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import tiktoken\n",
    "import sys\n",
    "\n",
    "def count_tokens(text, model=\"gpt-4o\"):\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def choose_file():\n",
    "    \"\"\"\n",
    "    Opens a file dialog for the user to select a txt file.\n",
    "    \"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the main Tkinter window\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Select a txt file\", \n",
    "        filetypes=[(\"Text Files\", \"*.txt\")]\n",
    "    )\n",
    "    return file_path\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = choose_file()\n",
    "    if not file_path:\n",
    "        print(\"No file selected. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            file_text = file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text file: {e}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Calculate token count using GPT‑4's \"cl100k_base\" encoding\n",
    "    token_count = count_tokens(file_text)\n",
    "    print(f\"Token count: {token_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17f9da6b-d1b4-458b-800a-c2ca6adac69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 1700\n",
      "Character count: 9074\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import tiktoken\n",
    "import sys\n",
    "\n",
    "def count_tokens(text, model=\"gpt-4o\"):\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def choose_file():\n",
    "    \"\"\"\n",
    "    Opens a file dialog for the user to select a txt file.\n",
    "    \"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the main Tkinter window\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Select a txt file\", \n",
    "        filetypes=[(\"Text Files\", \"*.txt\")]\n",
    "    )\n",
    "    return file_path\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = choose_file()\n",
    "    if not file_path:\n",
    "        print(\"No file selected. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            file_text = file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text file: {e}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Calculate token count using GPT‑4's \"cl100k_base\" encoding\n",
    "    token_count = count_tokens(file_text)\n",
    "    \n",
    "    # Calculate total characters\n",
    "    char_count = len(file_text)\n",
    "    \n",
    "    print(f\"Token count: {token_count}\")\n",
    "    print(f\"Character count: {char_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1886c9ec-4ed7-4717-ab69-1c4d25861c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 1700\n",
      "Character count: 9074\n",
      "Estimated cost: $0.0068\n"
     ]
    }
   ],
   "source": [
    " import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import tiktoken\n",
    "import sys\n",
    "\n",
    "def count_tokens(text, model=\"gpt-4o\"):\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def choose_file():\n",
    "    \"\"\"\n",
    "    Opens a file dialog for the user to select a txt file.\n",
    "    \"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the main Tkinter window\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Select a txt file\", \n",
    "        filetypes=[(\"Text Files\", \"*.txt\")]\n",
    "    )\n",
    "    return file_path\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = choose_file()\n",
    "    if not file_path:\n",
    "        print(\"No file selected. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            file_text = file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text file: {e}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Calculate token count using GPT-4's \"cl100k_base\" encoding\n",
    "    token_count = count_tokens(file_text)\n",
    "    \n",
    "    # Calculate total characters\n",
    "    char_count = len(file_text)\n",
    "    \n",
    "    # Cost calculation\n",
    "    price_per_1000_tokens = 0.004  # $0.004 per 1,000 tokens\n",
    "    equivalent_price = (token_count / 1000) * price_per_1000_tokens\n",
    "    \n",
    "    print(f\"Token count: {token_count}\")\n",
    "    print(f\"Character count: {char_count}\")\n",
    "    print(f\"Estimated cost: ${equivalent_price:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93c94b44-3daf-4188-9d3a-adc78e5f44b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 376\n",
      "Character count: 2159\n",
      "Estimated cost: $0.0015\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import tiktoken\n",
    "import sys\n",
    "\n",
    "def count_tokens(text, model=\"gpt-4o\"):\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        # Fallback to cl100k_base if the model isn't recognized\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def choose_file():\n",
    "    \"\"\"\n",
    "    Opens a file dialog for the user to select a txt file.\n",
    "    \"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the main Tkinter window\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Select a txt file\",\n",
    "        filetypes=[(\"Text Files\", \"*.txt\")]\n",
    "    )\n",
    "    return file_path\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = choose_file()\n",
    "    if not file_path:\n",
    "        print(\"No file selected. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            file_text = file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text file: {e}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Remove substrings that should not be counted\n",
    "    excluded_substrings = [\n",
    "        \"Ask about courses (or type 'exit' to quit)\",\n",
    "        \"Assistant: \"\n",
    "    ]\n",
    "    for ex_substring in excluded_substrings:\n",
    "        file_text = file_text.replace(ex_substring, \"\")\n",
    "    \n",
    "    # Calculate token count using GPT-4's \"cl100k_base\" encoding\n",
    "    token_count = count_tokens(file_text)\n",
    "    \n",
    "    # Calculate total characters\n",
    "    char_count = len(file_text)\n",
    "    \n",
    "    # Cost calculation\n",
    "    price_per_1000_tokens = 0.004  # $0.004 per 1,000 tokens\n",
    "    equivalent_price = (token_count / 1000) * price_per_1000_tokens\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Token count: {token_count}\")\n",
    "    print(f\"Character count: {char_count}\")\n",
    "    print(f\"Estimated cost: ${equivalent_price:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1450d047-215a-4a75-aebb-c4a452fc0464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 318\n",
      "Character count: 1599\n",
      "Estimated cost (new formula): $0.0032\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import tiktoken\n",
    "import sys\n",
    "\n",
    "def count_tokens(text, model=\"gpt-4o\"):\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        # Fallback to cl100k_base if the model isn't recognized\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def choose_file():\n",
    "    \"\"\"\n",
    "    Opens a file dialog for the user to select a txt file.\n",
    "    \"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the main Tkinter window\n",
    "    file_path = filedialog.askopenfilename(\n",
    "        title=\"Select a txt file\",\n",
    "        filetypes=[(\"Text Files\", \"*.txt\")]\n",
    "    )\n",
    "    return file_path\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = choose_file()\n",
    "    if not file_path:\n",
    "        print(\"No file selected. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            file_text = file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text file: {e}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Remove substrings that should not be counted\n",
    "    excluded_substrings = [\n",
    "        \"Ask about courses (or type 'exit' to quit)\",\n",
    "        \"Assistant: \"\n",
    "    ]\n",
    "    for ex_substring in excluded_substrings:\n",
    "        file_text = file_text.replace(ex_substring, \"\")\n",
    "    \n",
    "    # Calculate token count using GPT-4's \"cl100k_base\" encoding\n",
    "    token_count = count_tokens(file_text)\n",
    "    \n",
    "    # Calculate total characters\n",
    "    char_count = len(file_text)\n",
    "    \n",
    "    # New cost calculation using the provided formula:\n",
    "    # Cost_output = (token_count / 1,000,000) * 10\n",
    "    cost_output = (token_count / 1000000) * 10\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Token count: {token_count}\")\n",
    "    print(f\"Character count: {char_count}\")\n",
    "    print(f\"Estimated cost (new formula): ${cost_output:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e997adeb-3da4-4361-98d2-dfd91db2ff39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: Question10 Time2.txt\n",
      "Token count: 320\n",
      "Character count: 1819\n",
      "Estimated cost (using new formula): $0.0032\n",
      "----------------------------------------\n",
      "File: Question10 Time3.txt\n",
      "Token count: 311\n",
      "Character count: 1785\n",
      "Estimated cost (using new formula): $0.0031\n",
      "----------------------------------------\n",
      "File: Question10 Time1.txt\n",
      "Token count: 314\n",
      "Character count: 1776\n",
      "Estimated cost (using new formula): $0.0031\n",
      "----------------------------------------\n",
      "File: Question10 Time4.txt\n",
      "Token count: 323\n",
      "Character count: 1830\n",
      "Estimated cost (using new formula): $0.0032\n",
      "----------------------------------------\n",
      "File: Question10 Time5.txt\n",
      "Token count: 311\n",
      "Character count: 1770\n",
      "Estimated cost (using new formula): $0.0031\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import os\n",
    "import tiktoken\n",
    "import sys\n",
    "\n",
    "def count_tokens(text, model=\"gpt-4o\"):\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        # Fallback to cl100k_base if the model isn't recognized\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def choose_folder():\n",
    "    \"\"\"\n",
    "    Opens a folder dialog for the user to select a folder containing txt files.\n",
    "    \"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the main Tkinter window\n",
    "    folder_path = filedialog.askdirectory(title=\"Select folder containing txt files\")\n",
    "    return folder_path\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = choose_folder()\n",
    "    if not folder_path:\n",
    "        print(\"No folder selected. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # List all .txt files in the selected folder\n",
    "    txt_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.txt')]\n",
    "    if not txt_files:\n",
    "        print(\"No .txt files found in the selected folder.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    for txt_file in txt_files:\n",
    "        file_path = os.path.join(folder_path, txt_file)\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                file_text = file.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {txt_file}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Remove substrings that should not be counted\n",
    "        excluded_substrings = [\n",
    "            \"Ask about courses (or type 'exit' to quit)\",\n",
    "            \"Assistant: \"\n",
    "        ]\n",
    "        for ex_substring in excluded_substrings:\n",
    "            file_text = file_text.replace(ex_substring, \"\")\n",
    "        \n",
    "        # Calculate token count using GPT-4's \"cl100k_base\" encoding\n",
    "        token_count = count_tokens(file_text)\n",
    "        # Calculate total characters\n",
    "        char_count = len(file_text)\n",
    "        # Cost calculation using the provided formula:\n",
    "        # Cost_output = (token_count / 1,000,000) * 10\n",
    "        cost_output = (token_count / 1000000) * 10\n",
    "        \n",
    "        # Print results for each file\n",
    "        print(f\"File: {txt_file}\")\n",
    "        print(f\"Token count: {token_count}\")\n",
    "        print(f\"Character count: {char_count}\")\n",
    "        print(f\"Estimated cost (using new formula): ${cost_output:.4f}\")\n",
    "        print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27c36c53-91ca-4f07-9d46-305cff19ffc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: Question2_Time1.txt\n",
      "Token count: 251\n",
      "Character count: 1075\n",
      "Estimated cost (using new formula): $0.0025\n",
      "----------------------------------------\n",
      "File: Question2_Time3.txt\n",
      "Token count: 239\n",
      "Character count: 1010\n",
      "Estimated cost (using new formula): $0.0024\n",
      "----------------------------------------\n",
      "File: Question2_Time2.txt\n",
      "Token count: 240\n",
      "Character count: 1013\n",
      "Estimated cost (using new formula): $0.0024\n",
      "----------------------------------------\n",
      "File: Question2_Time6.txt\n",
      "Token count: 339\n",
      "Character count: 1455\n",
      "Estimated cost (using new formula): $0.0034\n",
      "----------------------------------------\n",
      "File: Question2_Time7.txt\n",
      "Token count: 244\n",
      "Character count: 1054\n",
      "Estimated cost (using new formula): $0.0024\n",
      "----------------------------------------\n",
      "File: Question2_Time5.txt\n",
      "Token count: 236\n",
      "Character count: 989\n",
      "Estimated cost (using new formula): $0.0024\n",
      "----------------------------------------\n",
      "File: Question2_Time4.txt\n",
      "Token count: 312\n",
      "Character count: 1344\n",
      "Estimated cost (using new formula): $0.0031\n",
      "----------------------------------------\n",
      "File: Question2_Time10.txt\n",
      "Token count: 353\n",
      "Character count: 1535\n",
      "Estimated cost (using new formula): $0.0035\n",
      "----------------------------------------\n",
      "File: Question2_Time9.txt\n",
      "Token count: 305\n",
      "Character count: 1294\n",
      "Estimated cost (using new formula): $0.0030\n",
      "----------------------------------------\n",
      "File: Question2_Time8.txt\n",
      "Token count: 298\n",
      "Character count: 1418\n",
      "Estimated cost (using new formula): $0.0030\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import os\n",
    "import tiktoken\n",
    "import sys\n",
    "\n",
    "def count_tokens(text, model=\"gpt-4o\"):\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        # Fallback to cl100k_base if the model isn't recognized\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def choose_folder():\n",
    "    \"\"\"\n",
    "    Opens a folder dialog for the user to select a folder containing txt files.\n",
    "    \"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the main Tkinter window\n",
    "    folder_path = filedialog.askdirectory(title=\"Select folder containing txt files\")\n",
    "    return folder_path\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = choose_folder()\n",
    "    if not folder_path:\n",
    "        print(\"No folder selected. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # List all .txt files in the selected folder\n",
    "    txt_files = [f for f in os.listdir(folder_path) if f.lower().endswith('.txt')]\n",
    "    if not txt_files:\n",
    "        print(\"No .txt files found in the selected folder.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    for txt_file in txt_files:\n",
    "        file_path = os.path.join(folder_path, txt_file)\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                file_text = file.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {txt_file}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Remove trailing blank lines at the bottom of the file while keeping internal blank lines\n",
    "        lines = file_text.splitlines(keepends=True)\n",
    "        while lines and not lines[-1].strip():\n",
    "            lines.pop()\n",
    "        file_text = ''.join(lines)\n",
    "        \n",
    "        # Remove substrings that should not be counted\n",
    "        excluded_substrings = [\n",
    "            \"Ask about courses (or type 'exit' to quit)\",\n",
    "            \"Assistant: \"\n",
    "        ]\n",
    "        for ex_substring in excluded_substrings:\n",
    "            file_text = file_text.replace(ex_substring, \"\")\n",
    "        \n",
    "        # Calculate token count using GPT-4's \"cl100k_base\" encoding\n",
    "        token_count = count_tokens(file_text)\n",
    "        # Calculate total characters\n",
    "        char_count = len(file_text)\n",
    "        # Cost calculation using the provided formula:\n",
    "        # Cost_output = (token_count / 1,000,000) * 10\n",
    "        cost_output = (token_count / 1000000) * 10\n",
    "        \n",
    "        # Print results for each file\n",
    "        print(f\"File: {txt_file}\")\n",
    "        print(f\"Token count: {token_count}\")\n",
    "        print(f\"Character count: {char_count}\")\n",
    "        print(f\"Estimated cost (using new formula): ${cost_output:.4f}\")\n",
    "        print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18b3a288-29fd-4265-92ca-b93080edb22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: Question3 Time6.txt\n",
      "Token count: 226\n",
      "Character count: 1210\n",
      "Estimated cost (using new formula): $0.0023\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import os\n",
    "import tiktoken\n",
    "import sys\n",
    "\n",
    "def count_tokens(text, model=\"gpt-4o\"):\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        # Fallback to cl100k_base if the model isn't recognized\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = encoding.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "def choose_files():\n",
    "    \"\"\"\n",
    "    Opens a file dialog for the user to select individual txt files.\n",
    "    \"\"\"\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()  # Hide the main Tkinter window\n",
    "    file_paths = filedialog.askopenfilenames(\n",
    "        title=\"Select txt files\", \n",
    "        filetypes=[(\"Text files\", \"*.txt\")]\n",
    "    )\n",
    "    return file_paths\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_paths = choose_files()\n",
    "    if not file_paths:\n",
    "        print(\"No files selected. Exiting.\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    for file_path in file_paths:\n",
    "        txt_file = os.path.basename(file_path)\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                file_text = file.read()\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {txt_file}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Remove trailing blank lines at the bottom of the file while keeping internal blank lines\n",
    "        lines = file_text.splitlines(keepends=True)\n",
    "        while lines and not lines[-1].strip():\n",
    "            lines.pop()\n",
    "        file_text = ''.join(lines)\n",
    "        \n",
    "        # Remove substrings that should not be counted\n",
    "        excluded_substrings = [\n",
    "            \"Ask about courses (or type 'exit' to quit)\",\n",
    "            \"Assistant: \"\n",
    "        ]\n",
    "        for ex_substring in excluded_substrings:\n",
    "            file_text = file_text.replace(ex_substring, \"\")\n",
    "        \n",
    "        # Calculate token count using GPT-4's \"cl100k_base\" encoding\n",
    "        token_count = count_tokens(file_text)\n",
    "        # Calculate total characters\n",
    "        char_count = len(file_text)\n",
    "        # Cost calculation using the provided formula:\n",
    "        # Cost_output = (token_count / 1,000,000) * 10\n",
    "        cost_output = (token_count / 1000000) * 10\n",
    "        \n",
    "        # Print results for each file\n",
    "        print(f\"File: {txt_file}\")\n",
    "        print(f\"Token count: {token_count}\")\n",
    "        print(f\"Character count: {char_count}\")\n",
    "        print(f\"Estimated cost (using new formula): ${cost_output:.4f}\")\n",
    "        print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8094538-60f2-4146-b593-a03a933f8bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your GPT-4o input:  Are there specific scheduling guidelines or typical semester offerings for required courses like Network Programming (CSC 544) and Database Management Systems (MIS 555) that I should plan for?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 35\n",
      "Total cost: $0.000087\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def calculate_gpt4o_input_cost(text: str, price_per_million_tokens: float = 2.50):\n",
    "    encoding = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "    token_count = len(encoding.encode(text))\n",
    "    cost = (token_count / 1_000_000) * price_per_million_tokens\n",
    "    return token_count, cost\n",
    "\n",
    "text_input = input(\"Enter your GPT-4o input: \")\n",
    "tokens, cost = calculate_gpt4o_input_cost(text_input)\n",
    "\n",
    "print(f\"Total tokens: {tokens}\")\n",
    "print(f\"Total cost: ${cost:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23cd4e82-2f3d-4602-b69c-70cacc490d09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 25566\n",
      "Total cost: $0.031957\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "\n",
    "def count_cached_input_tokens_and_cost(\n",
    "    jsonl_file_path: str,\n",
    "    model: str = \"gpt-4o\",\n",
    "    price_per_million_tokens: float = 1.25\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Reads a JSONL file line by line, uses tiktoken to count tokens,\n",
    "    and calculates cost based on the provided rate.\n",
    "    \n",
    "    :param jsonl_file_path: Path to your JSONL file\n",
    "    :param model: Model name for which you want to use tiktoken encoding\n",
    "    :param price_per_million_tokens: Price in dollars per 1 million tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the tiktoken encoding for the specified model\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with open(jsonl_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            # Strip empty lines/spaces\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            # Load JSON (if you need to process fields, do it here)\n",
    "            # data = json.loads(line)\n",
    "            # For counting tokens, you can encode the entire raw line as is:\n",
    "            tokens_in_line = len(encoding.encode(line))\n",
    "            total_tokens += tokens_in_line\n",
    "    \n",
    "    # Calculate cost\n",
    "    total_cost = (total_tokens / 1_000_000) * price_per_million_tokens\n",
    "    \n",
    "    print(f\"Total tokens: {total_tokens}\")\n",
    "    print(f\"Total cost: ${total_cost:.6f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    # Replace 'course_index_cache.jsonl' with the path to your JSONL file\n",
    "    jsonl_path = \"course_index_cache.json\"\n",
    "    count_cached_input_tokens_and_cost(jsonl_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "592135ef-9706-44dc-a113-6d6353372461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens: 25571\n",
      "Total cost: $0.031964\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "\n",
    "def count_cached_input_tokens_and_cost(\n",
    "    json_file_path: str,\n",
    "    model: str = \"gpt-4o\",\n",
    "    price_per_million_tokens: float = 1.25\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads a JSON file (single object or array of objects), uses tiktoken to count tokens,\n",
    "    and calculates cost based on the provided rate (default: $1.25 per 1M tokens).\n",
    "    \n",
    "    :param json_file_path: Path to your JSON file\n",
    "    :param model: Model name for which you want to use tiktoken encoding\n",
    "    :param price_per_million_tokens: Price in dollars per 1 million tokens\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the tiktoken encoding for the specified model\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    total_tokens = 0\n",
    "\n",
    "    # Load the entire JSON file\n",
    "    with open(json_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    def encode_item(item):\n",
    "        # Convert item to a JSON string to tokenize the entire structure\n",
    "        item_as_string = json.dumps(item, ensure_ascii=False)\n",
    "        return len(encoding.encode(item_as_string))\n",
    "\n",
    "    # If the JSON is a list, iterate over each element\n",
    "    if isinstance(data, list):\n",
    "        for item in data:\n",
    "            total_tokens += encode_item(item)\n",
    "    # If the JSON is a single dictionary or another type, just encode it directly\n",
    "    else:\n",
    "        total_tokens += encode_item(data)\n",
    "\n",
    "    # Calculate cost\n",
    "    total_cost = (total_tokens / 1_000_000) * price_per_million_tokens\n",
    "\n",
    "    print(f\"Total tokens: {total_tokens}\")\n",
    "    print(f\"Total cost: ${total_cost:.6f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    # Replace 'course_index_cache.json' with the path to your JSON file\n",
    "    json_path = \"course_index_cache.json\"\n",
    "    count_cached_input_tokens_and_cost(json_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4bfb00-8dab-4708-bcbe-7516470c42eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
