# Thesis-Work Repository

This repository contains the LaTeX source files for my **Masterâ€™s Thesis** and all associated code used for experimentation, data processing, and validation. The thesis investigates two GPT-4-based chatbot configurations (general-purpose vs. fine-tuned) for academic advising tasks, focusing on **token usage**, **computational costs**, and **domain-specific fine-tuning**.

> **Title of Thesis**  
> **ENHANCING COST EFFICIENCY AND PERFORMANCE OF GPT-4O CHATBOTS: A CASE STUDY ON ACADEMIC ADVISOR SYSTEMS**  
> **University of North Carolina Wilmington**  
> **Master of Degree in Computer Science and Information Systems**  
> *By Bulut Tok, 2025*

## Overview
This project explores **cost-efficiency** and **performance** optimizations in GPT-4O-based chatbots tailored for academic advising. The repository includes:

- **Thesis LaTeX Source** â€“ Full write-up, figures, tables, and references.  
- **Code for Catalog Parsing** â€“ Scripts to parse course data and produce JSON caches.  
- **Fine-Tuning and Training Scripts** â€“ Python scripts and `.jsonl` files for GPT-4O fine-tuning.  
- **Validation and Analysis** â€“ Jupyter notebooks or Python scripts used for testing output tokens, cost calculations, and final comparative analysis.  

## ğŸ“ Repository Structure

```
Thesis-Work/
â”œâ”€â”€ Base Model.ipynb               # Demo of the base GPTâ€‘4O model
â”œâ”€â”€ Fine Tuning Code.ipynb         # Notebook for the fineâ€‘tuning pipeline
â”œâ”€â”€ CatalogParsing.ipynb           # UNCW course catalogue parser
â”œâ”€â”€ Token_Calculations.ipynb       # Token usage & cost estimator
â”œâ”€â”€ course_index_cache.json        # Cache for course lookups
â”œâ”€â”€ training1.jsonl                # Domain-specific training data
â”œâ”€â”€ validation1.jsonl              # Validation dataset
â”œâ”€â”€ Thesis-Work-Latex-Document/    # LaTeX source for the full thesis
â”‚   â”œâ”€â”€ main.tex                   # Master document
â”‚   â”œâ”€â”€ chapters/                  # Individual chapter files (if split)
â”‚   â”œâ”€â”€ figures_current/                   # All figures and diagrams
â”‚   â””â”€â”€ references_thesis/                # .bib files and bibliography
â”œâ”€â”€ Questions-Asked/               # Test prompts and expected answers
â”œâ”€â”€ Token-Calculation-Code/        # Scripts & notebooks for deeper token analysis
â”œâ”€â”€ UNCW-2025-CATALOGUE-Parsing-Code/  # Parsing code for course catalog
â””â”€â”€ Training-and-Validation-Files/ # Supporting JSONL files for fineâ€‘tuning
```

---

## ğŸŒ¿ Branch Overview

* **main**: Core notebooks and data files (base model, fineâ€‘tuning, parsing, token calculations).
* **Thesis-Work-Latex-Document**: All LaTeX source (main.tex, chapters, figures, references).
* **Questions-Asked**: Test suite of identical evaluation questions and expected outputs.
* **Token-Calculation-Code**: Inâ€‘depth analysis scripts for token usage and cost breakdowns.
* **UNCW-2025-CATALOGUE-Parsing-Code**: Course catalog extraction and transformation pipeline.
* **Training-and-Validation-Files**: JSONL files and notebooks for creating & validating fineâ€‘tuning data.

---

### ğŸ” Branch Details

#### Question-Based Chatbot (Question-Based-LLM)
Contains Jupyter notebooks for the question-by-question chatbot workflow.  
- **Purpose:** Processes user queries in a Q&A format while measuring per-question token usage and cost.  
- **Key Files:**  
  - `Question-Based-Chatbot.ipynb` â€” Main notebook for interactive, question-driven analysis.  
  - `Token_Calculations.ipynb`      â€” Notebook that computes token counts and per-question cost.  

#### Thesis Assembly Source (Thesis-Collation-Code)
Organizes LaTeX source files, figures, and bibliography for thesis compilation.  
- **Purpose:** Assembles the complete thesis into a single PDF document.  
- **Key Files:**  
  - `main.tex`                      â€” Master LaTeX file that includes all chapters.  
  - `chapters/`                     â€” Contains individual chapter files (e.g., `introduction.tex`, `methodology.tex`).  
  - `figures/`                      â€” Figures and diagrams used throughout the thesis.  

#### Catalog Parsing Notebooks (IMDC-2023-CATALOG-Parsing-Code)
Parses university course catalog data into structured JSON.  
- **Purpose:** Converts raw PDF/HTML course listings into clean, queryable JSON formats.  
- **Key Files:**  
  - `CatalogParsing.ipynb`          â€” Jupyter notebook for extracting and structuring course data.  
  - `course_index_cache.json`       â€” Example JSON cache of parsed course metadata.  

#### Fine-Tuning & Validation Data (Training-and-Validation-Files)
Stores training and validation datasets along with fine-tuning scripts.  
- **Purpose:** Supports domain-specific fine-tuning and performance evaluation of GPT-4O.  
- **Key Files:**  
  - `training1.jsonl`               â€” JSONL file with training examples for fine-tuning.  
  - `validation1.jsonl`             â€” JSONL file containing validation samples.  
  - `Fine Tuning Code.ipynb`        â€” Notebook illustrating the fine-tuning process via the OpenAI API.
  '''
---

## ğŸš€ Getting Started

1. **Clone the repository**

   ```bash
   git clone git@github.com:BulutTok/Thesis-Work.git
   cd Thesis-Work
   ```

2. **Switch to the branch you need**

   ```bash
   git checkout Thesis-Work-Latex-Document
   # or any other branch
   ```

3. **Install dependencies** (Python, Jupyter, LaTeX)

   ```bash
   pip install -r requirements.txt      # Notebook dependencies
   # Ensure a LaTeX distribution (TeX Live or MikTeX) is installed
   ```

4. **Work with notebooks**

   ```bash
   jupyter lab                          # Launch Jupyter Lab
   ```

5. **Compile the thesis PDF**

   ```bash
   cd Thesis-Work-Latex-Document
   pdflatex main.tex
   bibtex main
   pdflatex main.tex
   pdflatex main.tex
   ```

---

## ğŸ“§ Contact

* **Author:** Bulut Tok
* **Email:** [buluttok2013@gmail.com](mailto:buluttok2013@gmail.com)
* **Affiliation:** UNCW, MS in Computer & Information Science (Ê¹25)


## ğŸ“Š Key Results

This section highlights quantitative findings from both the base and fineâ€‘tuned GPTâ€‘4O models, including perâ€‘question token usage, cost comparisons, and scenarioâ€‘based savings.

### 10.1 Comparative Cost Efficiency of Base vs. Fineâ€‘Tuned Models

| Metric                   | Base Model | Fineâ€‘Tuned Model | Reduction (%) |
| ------------------------ | ---------- | ---------------- | ------------- |
| Total Output Tokens      | 39,353     | 26,960           | 31.5%         |
| Total Cost (USD)         | \$0.39353  | \$0.26960        | 31.5%         |
| Average Tokens per Q     | 3,935.3    | 2,696.0          | 31.5%         |
| Average Cost per Q (USD) | \$0.039353 | \$0.026960       | 31.5%         |

* **Response Consistency:** Box plots (Figure 10.1) show median outputâ€‘token counts per question dropping substantially after fineâ€‘tuning (e.g., Q4 from \~460 to \~240 tokens, Q6 from \~300 to \~170) and interquartile ranges narrowing.
* **Perâ€‘Question Efficiency:** Bar charts (Figure 10.2) illustrate consistent token reductions across all ten questions, with the largest gains in Q4 (âˆ’203.1 tokens), Q5 (âˆ’144.0 tokens), and Q10 (âˆ’180.5 tokens).
* **Cost Comparison:** Cost charts (Figure 10.3) reflect lower average perâ€‘question spending for the fineâ€‘tuned model (e.g., Q1: \$0.00302 â†’ \$0.00239; Q4: \$0.00460 â†’ \$0.00257).

### 10.2 Scenarioâ€‘Based Cost Savings Analysis

Using engagement rates from Georgia State University and projected queries for a 75â€‘student cohort (3.5 queries/user/month), we estimate:

| Month     | Active Users | Queries | Base Cost (USD) | Tuned Cost (USD) | Savings (USD) |
| --------- | ------------ | ------- | --------------- | ---------------- | ------------- |
| September | 30           | 105     | \$4.13          | \$2.83           | \$1.30        |
| October   | 25           | 88      | \$3.46          | \$2.37           | \$1.09        |
| November  | 35           | 123     | \$4.84          | \$3.32           | \$1.52        |
| December  | 20           | 70      | \$2.75          | \$1.89           | \$0.87        |
| **Total** | â€”            | **386** | **\$15.19**     | **\$10.41**      | **\$4.78**    |

* **Perâ€‘Query Savings:** Average savings of \$0.01239 per query (31.5%).
* **Semester Impact:** Fineâ€‘tuning yields \$4.78 saved over 386 queriesâ€”funds that can be redirected to humanâ€‘inâ€‘theâ€‘loop review or further enhancements.

These results demonstrate that domainâ€‘specific fineâ€‘tuning substantially reduces token usage and costs while improving response consistency, making it highly beneficial for scalable academicâ€‘advising deployments.






### Question-based-LLM
Contains code related to the question-by-question chatbot approach.  
- **Purpose**: Demonstrates how user queries are processed in a Q&A format, focusing on cost measurements for each question.  
- **Key Files**:  
  - `scripts/question_based_chatbot.py` â€“ The main script for question-driven interactions.  
  - `analysis/token_calculations.ipynb` â€“ A notebook that calculates per-question token usage.  

### Thesis-Collation-Code
Collects LaTeX files, figure references, and the final structure used to build the thesis PDF.  
- **Purpose**: This is the â€œmasterâ€ branch for assembling the thesis text.  
- **Key Files**:  
  - `main.tex` â€“ Main LaTeX file.   
  - `figures/` â€“ Figures used throughout the thesis.  

### IMDC-2023-CATALOG-Parsing-Code
Holds the scripts that parse the 2023 course catalog (or any relevant university course data) into JSON format.  
- **Purpose**: To transform raw PDF or HTML-based course listings into structured JSON data.  
- **Key Files**:  
  - `parse_catalog.py` â€“ Python script that extracts course data from PDF/HTML.  
  - `catalog_cache.json` â€“ Example JSON output for course info.  

### Training-and-Validation-Files
Stores the `.jsonl` files for training, validation, and fine-tuning of GPT-4O.  
- **Purpose**: Central location for your **fine-tuning** data and scripts.  
- **Key Files**:  
  - `training_data.jsonl` â€“ Training examples for domain-specific customization.  
  - `validation_data.jsonl` â€“ Held-out set for validation metrics.  
  - `fine_tune_script.py` â€“ Example script to run the fine-tuning process via OpenAIâ€™s API.  

---

## Thesis Highlights
1. **Comparative Analysis** â€“ Evaluates a general-purpose GPT-4O vs. a domain-specific fine-tuned GPT-4O.  
2. **Cost & Token Usage** â€“ Investigates how domain-specific fine-tuning can reduce tokens and thus operational costs.  
3. **Academic Advising Context** â€“ Focuses on how chatbots can provide **degree audits**, **course recommendations**, and **personalized** academic advice.  
4. **Quantitative & Qualitative Metrics** â€“ Employs token counts, cost analysis, and human evaluation (relevance, clarity) to measure performance.

---

## Getting Started

### Prerequisites
- **Python 3.8+**  
- **LaTeX Distribution** (e.g., TeX Live, MikTeX) for compiling the thesis  
- Libraries (install via `pip install -r requirements.txt`):
  - `PyPDF2`, `pandas`, `numpy`, `openai`, `tiktoken`, `nltk`, etc.  

### Installation
1. **Clone the repository**:
   ```bash
   git clone https://github.com/YourUsername/Thesis-Work.git
   ```
2. **Navigate to the project directory**:
   ```bash
   cd Thesis-Work
   ```
3. **Set up a virtual environment** *(optional but recommended)*:
   ```bash
   python -m venv venv
   source venv/bin/activate  # Linux/Mac
   venv\Scripts\activate     # Windows
   ```
4. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

---

## Usage

### Compiling the Thesis (LaTeX)
1. Go to the `Thesis-Collation-Code` branch:
   ```bash
   git checkout Thesis-Collation-Code
   ```
2. Compile using your preferred LaTeX tool (e.g., `pdflatex`, `xelatex`, or an IDE like TeXStudio):
   ```bash
   pdflatex main.tex
   bibtex main
   pdflatex main.tex
   pdflatex main.tex
   ```
3. The output PDF (e.g., `main.pdf`) should appear in the same directory.

### Running the Code

1. **Catalog Parsing** (IMDC-2023-CATALOG-Parsing-Code branch):  
   ```bash
   git checkout IMDC-2023-CATALOG-Parsing-Code
   python parse_catalog.py
   ```
   - This generates a structured JSON file of course data.

2. **Fine-Tuning** (Training-and-Validation-Files branch):  
   ```bash
   git checkout Training-and-Validation-Files
   python fine_tune_script.py --train_file training_data.jsonl --valid_file validation_data.jsonl
   ```
   - Adjust arguments as needed for your environment.

3. **Question-Based LLM** (Question-based-LLM branch):  
   ```bash
   git checkout Question-based-LLM
   python question_based_chatbot.py
   ```
   - Interactively ask academic advising questions, measure token usage, and compare costs.

---

## Data and Fine-Tuning
- **Fine-Tuning Data**: Found in `Training-and-Validation-Files` branch. Contains domain-specific Q&A examples in `.jsonl` format.  
- **Scripts**: Demonstrates how we upload data to OpenAI, parse logs for training loss, and compare the final fine-tuned model with the base GPT-4O model.

---

## Results and Evaluation
- **Token Usage**: Detailed logs for both base and fine-tuned models are saved in `.csv` or `.ipynb` files.  
- **Cost Analysis**: Compares input vs. output tokens and calculates cost with OpenAIâ€™s pricing model.  
- **Evaluation Figures**: Plots, bar charts, and box-whisker diagrams found under `analysis/` or embedded in the thesis.

---

## Contributing
Contributions, suggestions, or bug reports are welcome!  

1. **Fork** the repository.  
2. **Create a new branch** for your feature/bugfix (`git checkout -b feature/YourFeature`).  
3. **Commit** your changes (`git commit -m 'Add feature'`).  
4. **Push** to your branch (`git push origin feature/YourFeature`).  
5. Open a **Pull Request** describing your changes.

Please ensure that any code or documentation adheres to the repositoryâ€™s style guidelines.

---


* **Author:** Bulut Tok
* **Email:** [buluttok2013@gmail.com](mailto:buluttok2013@gmail.com)
* **Affiliation:** UNCW, MS in Computer & Information Science (Ê¹25)

---

Feel free to explore each branch for focused development, raise issues, or contribute via pull requests. Good luck and happy researching! ğŸ“

---
